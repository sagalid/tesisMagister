\subsection{Metaheuristics and their classifications }
The $meta$ and $heuristic$ are Greek words, $meta$ it means $higher level$ and $heuristics$ means $to$ $find$, $to$ $know$ or $to$ $discover$. Metaheuristics are a set of intelligent strategies to enhance the efficiency of heuristic procedures.\\
A metaheuristic will be successful on a given optimizaction problem if it can provide a balance between exploration and exploitation. Exploration means to generate diverse solutions, associating them to different points of the search space on the global scale, while exploitation means to search in a local region by exploiting the information that a current good solution has been found in this region.
Exploration is often directed by use of randomization, which enables an algorithm to have the ability to jump between different bounded spaces and gives to the algorithm the capability to search optimal solutions globally.\\

The different metaheuristic approaches can be characterized by different aspects concerning the search path they follow or how search space is exploited \cite{citeulike:1859945}. The majority of these algorithms has a stochastic behavior and mimics biological or  physical processes. This presented classification was proposed by Beheshti et al. \cite{Beheshti:2014:CCA:2563733.2564085}, and can be understood better as shown in the (Figure \ref{fig:classification-of-mh}).

\squeezeup
\begin{figure}[ht] %ht
	\centering
  \includegraphics[width=0.70\textwidth]{MarcoTeorico/imagenes/classification_mh.png}
	\caption{Classification of metaheuristic}\label{fig:classification-of-mh}
\end{figure}
\squeezeup


~\\
\textbf{Nature-inspired vs. non-nature inspiration }
This class is based on the origin of algorithm. The majority of meta-heuristics are nature-inspired algorithms such as Black Hole (BH) \cite{Rubio2016}, Particle Swarm Optimization (PSO) \cite{Duran:2010:CPS:1645454.1645859} and Genetic Algorithms (GA) \cite{DBLP:conf/icsi/CrawfordSPJPO14}. Also, some of them are non-nature-inspired algorithms like Iterated Local Search (ILS) \cite{DBLP:journals/networks/AringhieriGHS16} and TabuSearch (TS) \cite{DBLP:journals/eswa/SotoCGMP13}.

~\\
\textbf{Population-based vs. single-point search}
There is a certain group of metaheuristics, that can be classified by the number of solutions in their lifecycle or execution, such as Trajectory methods, which are algorithms working based on a single solution at any time (Figure \ref{fig:trajectory-method}). On the other hand, Population-based algorithms perform searches with multiple initial points in a parallel style. Examples of these metaheuristics can be: Harmony Search (HS) \cite{DBLP:conf/ccece/Al-AjmiE14}, GA \cite{Aupetit2008} and PSO. 

\squeezeup
\begin{figure}[ht]
	\centering
  \includegraphics[width=0.4\textwidth]{MarcoTeorico/imagenes/trajectory-mh.png}
	\caption{Trajectory-based method}\label{fig:trajectory-method}
\end{figure}
\squeezeup

~\\
\textbf{Dynamic vs. static objective function}
Another way of classifying metaheuristics, is by the way of utilizing the objective function. Some algorithms maintain the objective function intact throughout the execution cycle, while others modify the objective function according to information collected at runtime.\\
An example of the second case presented, is Guided Local Search (GLS) \cite{DBLP:journals/eor/VansteenwegenSBO09}.  The idea behind this approach is to escape from local optima by changing the search landscape. 

~\\
\textbf{One vs. various neighborhood structures}
The majority of metaheuristic algorithms apply one single neighborhood structure. The fitness landscape topology remain unaltered thru the course of the algorithm while others, like Variable Neighborhood Search (VNS) \cite{DBLP:journals/anor/SarasolaDSA16}, employ a set of neighborhood structures. This latter structure gives the possibility to diversify the search by swapping between different fitness landscapes.

~\\
\textbf{Memory usage vs. memoryless methods}
One of the most interesting variables to classify a metaheuristic is undoubtedly use of memory. Short term usually is different from long term memory. The first kind usually keeps track of recently performed moves, or taken decisions. The second is usually an accumulation of synthetic parameters about the search. 

\subsection{Development Framework}
In an effort to maintain consistency in the structure of the proposed metaheuristic development, a well-defined model is followed, in order to structure the development steps of the proposed technique (Figure \ref{fig:GuideSolvingSCP}). \\
~\\
At first step, the problem is modeled and, based on its properties, application requirements are defined. In the particular case of this investigation the SCP is defined and validated against instances of benchmarck. The number of iterations has been considered, but runtime has not.\\

The second step basically corresponds to design the metaheuristic. In the case of this research, problems will be treated with variants of Harmony Search. That is to say not designed from the ground up, but certain elements are incorporated to improve the algorithm standard behavior. These elements are selected once known the nature of the problem to be solved and the metaheuristic to be used. Elements like the objective function to use and classification (Population based) are set by default.\\

The third step is to adopt a strategy of development of the technique or metaheuristic. In this research, it has been chosen to develop metaheruistics from scratch, thereby achieving maximum control of it. It was used for this purpose Python 2.7 as programming language and  PyCharm as IDE.\\

The fourth step is handling optimization parameters. There are two types of parameter tuning: Online and Off line. In this research, management of both types is performed to achieve good results. Specifically improving the metaheuristic proposal is based on the dynamic variation of the parameter that generates solutions. The following sections will go into detail on this subject.\\

The fifth and final step is to design experiments, get results and compare improvements. Then perform changes in parameters or operators so as to keep improving convergence, achieving an optimum balance between exploration and exploitation. This framework adopts an iterative approach because when step five is completed, systems can return to step one, two or three to seek a better solution.